{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dff9fb7",
   "metadata": {},
   "source": [
    "# About\n",
    "- Mostly copy paste, so still need to modify functions, paths...\n",
    "- PROBLEM: we don't have any ground truth data... need to discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ba04b",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pillow matplotlib scikit-image scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b675b",
   "metadata": {},
   "source": [
    "# Load libraries & define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57368384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imsave, imread\n",
    "from skimage.segmentation import slic\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"data/\" # this is the path to the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da47d4",
   "metadata": {},
   "source": [
    "# 1. Extract regions and features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db456e21",
   "metadata": {},
   "source": [
    "### Functions to calculate regions and feature values \n",
    "- TO MODIFY: depening on number of bands and format of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_shape_pixels_by_bands(data): # SEE IF NEEDED\n",
    "    num_dimensions = len(data.shape)\n",
    "    assert(num_dimensions == 2 or num_dimensions == 3)\n",
    "    if num_dimensions == 3:\n",
    "        num_bands = data.shape[2]\n",
    "        return data.reshape((-1, num_bands))\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def compute_average_feature(data):\n",
    "    # If needed convert data to the shape (num_pixels x num_bands)\n",
    "    data_2d = convert_to_shape_pixels_by_bands(data)\n",
    "    # Get the number of bands\n",
    "    num_bands = data_2d.shape[1]\n",
    "    avg_features = np.zeros(num_bands)\n",
    "    for b in range(num_bands):\n",
    "        # Compute the average value of each band (use the function np.mean)\n",
    "        avg_features[b] = np.mean(data_2d[:, b])\n",
    "    return avg_features\n",
    "\n",
    "def compute_standard_deviation_feature(data):\n",
    "    # If needed convert data to the shape (num_pixels x num_bands)\n",
    "    data_2d = convert_to_shape_pixels_by_bands(data)\n",
    "    # Compute the standard deviation feature (using the numpy function np.std)\n",
    "    #       as in the function compute_average_feature iterate over the bands\n",
    "    #       and compute one value for each band\n",
    "    num_bands = data_2d.shape[1]\n",
    "    avg_features = np.zeros(num_bands)\n",
    "    for b in range(num_bands):\n",
    "        avg_features[b] = np.std(data_2d[:, b])\n",
    "    return avg_features\n",
    "\n",
    "def compute_histogram_feature(data, num_bins=10):\n",
    "    # If needed convert data to the shape (num_pixels x num_bands)\n",
    "    data_2d = convert_to_shape_pixels_by_bands(data)\n",
    "    num_bands = data_2d.shape[1]\n",
    "    hist_features = np.zeros((num_bands, num_bins)).astype(np.float32)\n",
    "    for b in range(num_bands):\n",
    "        # Compute the histogram for each band \n",
    "        #       use the function np.histogram(array, bins=num_bins)\n",
    "        hist, boundaries = np.histogram(data_2d[:, b], bins=num_bins)\n",
    "        hist_features[b, :] = hist\n",
    "    # Return a 1D array containing all the values\n",
    "    return hist_features.flatten()\n",
    "\n",
    "def compute_image_features_from_regions(image, segmentation_map):\n",
    "    num_regions = len(np.unique(segmentation_map))\n",
    "    all_features = []\n",
    "    for id_region in range(num_regions):\n",
    "        # Obtain pixel values of each regions, with shape (num_pixels x num_bands)\n",
    "        pixel_values = image[segmentation_map==id_region]\n",
    "        # Compute the average, standard deviation and histogram features\n",
    "        #       and concatenated them unsing the function (np.concatenate)\n",
    "        avg = compute_average_feature(pixel_values)\n",
    "        features = compute_standard_deviation_feature(pixel_values)\n",
    "        hist_features = compute_histogram_feature(pixel_values)\n",
    "        features = np.concatenate([avg, features, hist_features])\n",
    "        # Add concatenated features to the variable all_features\n",
    "        all_features.append(features)\n",
    "    # convert list to numpy array of shape: (num_regions x num_bands)\n",
    "    return np.array(all_features).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357e609",
   "metadata": {},
   "source": [
    "### Compute for all our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bdc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_images = ... \n",
    "\n",
    "for image_number in range(1, total_num_images + 1):\n",
    "    # Read image\n",
    "    image_path = INPUT_DIR + \"images/\" + ... # add path name\n",
    "    image = imread(image_path)\n",
    "    # Segment image using SLIC (use the parameter n_segments=1000)\n",
    "    segmented_image = slic(image, n_segments=1000, start_label=0)\n",
    "    # Save SLIC regions\n",
    "    regions_path = INPUT_DIR + \"regions/\" + ... # add path name\n",
    "    imsave(regions_path, segmented_image.astype(np.uint32))\n",
    "    print(\"Regions saved in \" + regions_path)\n",
    "    # Compute features\n",
    "    region_features = compute_image_features_from_regions(image, segmented_image)\n",
    "    # Save features\n",
    "    region_features_path = INPUT_DIR + \"features/\" +... # add path name\n",
    "    np.save(region_features_path, region_features)\n",
    "    print(\"Features saved in \" + region_features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732fb1c2",
   "metadata": {},
   "source": [
    "# 2. Create training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac893ee0",
   "metadata": {},
   "source": [
    "### Split dataset into train/validation/test\n",
    "- TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining which images will belong to the train, test, and validation sets\n",
    "train_images = [...] \n",
    "test_images = [...]\n",
    "validation_images = [..]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3aac76",
   "metadata": {},
   "source": [
    "### Function to get labels per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_per_region(segmented_image, label_map):\n",
    "    \"\"\"\n",
    "    Returns a 1D numpy array that contains the label for each region, shape: (num_regions)\n",
    "            For each region, we obtain the label that has the largest intersection with it\n",
    "    \"\"\"\n",
    "    num_regions = len(np.unique(segmented_image))\n",
    "    num_labels = len(np.unique(label_map))\n",
    "    region_labels = []\n",
    "    for region_id in range(num_regions):\n",
    "        mask_region = segmented_image == region_id #Select a region. Gives a matrix with True/False\n",
    "        \n",
    "        intersection_per_label = []\n",
    "        for label_id in range(num_labels):\n",
    "            mask_label = label_map == label_id #Select a label\n",
    "            # Compute intersection of each region with each label\n",
    "            intersection = np.sum(mask_region * mask_label) # We look in label only where region mask is true. Then we count number of each label with the sum.\n",
    "            intersection_per_label.append(intersection)\n",
    "        \n",
    "        intersection_per_label = np.array(intersection_per_label)\n",
    "        # Obtain the index of the label with largest intersection\n",
    "        selected_label = np.argmax(intersection_per_label)\n",
    "        region_labels.append(selected_label)\n",
    "    \n",
    "    return np.array(region_labels).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe78c7a",
   "metadata": {},
   "source": [
    "### Create training targets and features \n",
    "TO CHANGE:\n",
    "- We don't have the ground truth: need to compute it ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_region_features = []\n",
    "all_train_region_labels = []\n",
    "\n",
    "for image_number in train_images: # to adapt\n",
    "    # Read segmented image\n",
    "    segmented_image_path = INPUT_DIR + \"regions/\" + ... \n",
    "    segmented_image = imread(segmented_image_path)\n",
    "    # Read ground truth image\n",
    "    gt_path = INPUT_DIR + \"gt/\" + ...\n",
    "    gt_image = imread(gt_path)\n",
    "    # TODO: Get labels per region using the function \"get_label_per_region\" defined above\n",
    "    region_labels = get_label_per_region(segmented_image, gt_image)\n",
    "    # Add current region labels to the variable all_train_region_labels\n",
    "    all_train_region_labels.append(region_labels)\n",
    "    # Read features using the function: np.load(file_path.npy)\n",
    "    region_features_path = INPUT_DIR + \"features/\" + ...\n",
    "    region_features = np.load(region_features_path)\n",
    "    # Add current region features to the variable all_train_region_features\n",
    "    all_train_region_features.append(region_features)\n",
    "\n",
    "# Tranforming the list all_train_region_labels in an array of shape: (num_all_regions)\n",
    "train_labels = np.concatenate(all_train_region_labels)\n",
    "print(\"train_labels shape \" + str(train_labels.shape))\n",
    "# Tranforming the list all_train_region_features in an array of shape: (num_all_regions, num_features)\n",
    "train_features = np.concatenate(all_train_region_features)\n",
    "print(\"train_features shape \" + str(train_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100fa8a",
   "metadata": {},
   "source": [
    "# 3. Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64268de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_per_feature = np.mean(train_features, axis=0)\n",
    "std_per_feature = np.std(train_features, axis=0)\n",
    "norm_train_features = (train_features - mean_per_feature) / std_per_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50800e2e",
   "metadata": {},
   "source": [
    "# 4. Train a Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75eb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=10)\n",
    "classifier.fit(norm_train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5f41b",
   "metadata": {},
   "source": [
    "# 5. Function to prediction classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classification_maps_and_get_accuracy(classifier, image_numbers, input_dir, save_predictions=False):\n",
    "    list_predictions = []\n",
    "    list_gt_labels = []\n",
    "    for image_number in image_numbers:\n",
    "        # Read segmented image\n",
    "        segmented_image_path = INPUT_DIR + \"regions/\" + ...\n",
    "        segmented_image = imread(segmented_image_path)\n",
    "        # Read features using the function: np.load(file_path.npy)\n",
    "        region_features_path = INPUT_DIR + \"features/\" + ...\n",
    "        region_features = np.load(region_features_path)\n",
    "        # Normalize features by substracting the mean and dividing by the standard deviation (of the train set) \n",
    "        norm_region_features = (region_features - mean_per_feature) / std_per_feature\n",
    "        # Predict label of regions \n",
    "        label_predictions = classifier.predict(norm_region_features)\n",
    "        # Compute label predictions per pixel\n",
    "        predicion_map = np.zeros(segmented_image.shape).astype(np.uint8)\n",
    "        num_regions = len(np.unique(segmented_image))\n",
    "        for region_id in range(num_regions):\n",
    "            # Set the label predicted for a region to all the pixels of that region\n",
    "            predicion_map[segmented_image==region_id] = label_predictions[region_id]\n",
    "        \n",
    "        list_predictions.append(predicion_map.flatten())\n",
    "        \n",
    "        if save_predictions:\n",
    "            # Save prediction map image\n",
    "            prediction_map_path = INPUT_DIR + \"prediction_map_\" + ...\n",
    "            imsave(prediction_map_path, predicion_map)\n",
    "        \n",
    "        # Get ground truth label\n",
    "        gt_path = INPUT_DIR + \"gt/\" + ...\n",
    "        gt = imread(gt_path)\n",
    "        \n",
    "        list_gt_labels.append(gt.flatten())\n",
    "    \n",
    "    all_predictions = np.concatenate(list_predictions)\n",
    "    all_gt_labels = np.concatenate(list_gt_labels)\n",
    "    accuracy = accuracy_score(all_gt_labels, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_gt_labels, all_predictions)\n",
    "                     \n",
    "    return accuracy, conf_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix):\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.5)\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='x-large')\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Ground Truth', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de0c7ca",
   "metadata": {},
   "source": [
    "# 6. Computer performance on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4716ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_val, conf_matrix_val = predict_classification_maps_and_get_accuracy(classifier, validation_image_numbers, input_dir)\n",
    "print(acc_val)\n",
    "print(conf_matrix_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450fcc4a",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1612f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of paramaters to evaluate in the validation set\n",
    "n_estimators_values = [20, 50, 100, 200]\n",
    "max_depth_values = [5, 10, 20, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise best_val_accuracy, best_depth and best_n_estimators\n",
    "best_val_accuracy = -1\n",
    "best_depth = None\n",
    "best_n_estimators = None\n",
    "\n",
    "for n_estimators_val in n_estimators_values:\n",
    "    for max_depth_val in max_depth_values:\n",
    "        print(\"eval n_estimators_val {} max_depth_val {}\".format(n_estimators_val, max_depth_val))\n",
    "        classifier = RandomForestClassifier(random_state=10, n_estimators=n_estimators_val, max_depth=max_depth_val)\n",
    "        classifier.fit(norm_train_features, train_labels)\n",
    "        val_acc, conf_matrix_val = predict_classification_maps_and_get_accuracy(classifier, validation_image_numbers, input_dir)\n",
    "        \n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_depth = max_depth_val\n",
    "            best_n_estimators = n_estimators_val\n",
    "            print(val_acc)\n",
    "            \n",
    "print(\"Best max depth: \" +str(best_depth))\n",
    "print(\"Best number estimators: \"+str(best_n_estimators))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fca624",
   "metadata": {},
   "source": [
    "# 8. Train final model & predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=10, n_estimators=best_n_estimators, max_depth=best_depth)\n",
    "classifier.fit(norm_train_features, train_labels)\n",
    "accuracy, conf_matrix = predict_classification_maps_and_get_accuracy(classifier, test_image_numbers, input_dir, save_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc_test, best_conf_matrix_test = predict_classification_maps_and_get_accuracy(classifier, test_image_numbers, input_dir, save_predictions=True)\n",
    "print(best_acc_test)\n",
    "print(best_conf_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2b7e0",
   "metadata": {},
   "source": [
    "# 9. View results --> modify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9aee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_label_image(label_map):\n",
    "    colors = np.array([[255, 255, 255], # Impervious: white\n",
    "                       [0, 0, 255],     # Building: Blue\n",
    "                       [0, 255, 255],   # Low vegetation: cyan\n",
    "                       [0, 255, 0]])    # Tree: green\n",
    "    colors = colors.astype(np.uint8)\n",
    "    color_map = np.zeros((label_map.shape[0], label_map.shape[1], 3)).astype(np.uint8)\n",
    "    for label_id in range(colors.shape[0]):\n",
    "        color_map[label_map == label_id] = colors[label_id, :]\n",
    "    plt.figure(figsize = (11,11)) \n",
    "    plt.imshow(color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_map_path = input_dir + \"prediction_map_\" +str(4) + \".tif\" #need to change image number and select one of the saved ones\n",
    "prediction_map = imread(prediction_map_path)\n",
    "display_label_image(prediction_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3e67f",
   "metadata": {},
   "source": [
    "# 10. Compute ice melting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038b7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
